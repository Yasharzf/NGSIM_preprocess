{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NGSIM_preprocess_lead_follower.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasharzf/NGSIM_preprocess/blob/master/Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCSfzUk2yx3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Colab notebook to preprocess NGSIM dataset to extract leading and following\n",
        "vehicle information. \n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bisect\n",
        "import json\n",
        "import time "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzztNlZSWVbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################  Loading  NGSIM ########################\n",
        "def load_data(download=True, path_to_file=None):\n",
        "    \"\"\"load NGSIM dataset.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    download : boolean\n",
        "        - False: if NGSIM dataset is loaded in google drive and it should be\n",
        "        mounted to colab. path_to_file is required.\n",
        "        - True: NGSIM should be downloaded from the website\n",
        "    path_to_file : str\n",
        "        Path to file in google drive. This is required if mounted is true.\n",
        "    \n",
        "    Returns:\n",
        "    -------\n",
        "    data : pandas.Dataframe\n",
        "        pandas dataframe containing NGSIM dataset.\n",
        "    \"\"\"\n",
        "    print(\"#### Mounting google colab ###########\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    if download:\n",
        "        print(\"#### Downloading NGSIM dataset #######\")\n",
        "        !wget --content-disposition https://data.transportation.gov/api/views/8ect-6jqj/rows.csv?accessType=DOWNLOAD\n",
        "        path_to_file = '/content/'\n",
        "\n",
        "    data = pd.read_csv(path_to_file + \"Next_Generation_Simulation__NGSIM__Vehicle_Trajectories_and_Supporting_Data.csv\")\n",
        "    \n",
        "    # drop duplicate, RAW NGSIM HAS DUPLICATED ROWS     \n",
        "    # example: data[(data.Vehicle_ID==515) & (data.Global_Time==1118848075000)]\n",
        "    data = data.drop_duplicates(subset=['Vehicle_ID', 'Global_Time'])\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHOpVWsOXrom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################### Lead Info ################################\n",
        "\n",
        "def convert_to_dict(data):\n",
        "    \"\"\"Converts pandas dataframe into a dictionay.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.Dataframe\n",
        "        pandas dataframe containing NGSIM dataset\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    data_dict : dict\n",
        "        a dictinoary containing NGSIM data\n",
        "    \"\"\"\n",
        "    print(\"=== Coverting relavent data to dictionary ===\")\n",
        "    data_dict = {}\n",
        "    for index, data_i in data.iterrows():\n",
        "        if index % 50000 == 0:\n",
        "            print('index: ', index)\n",
        "        if data_i[\"Vehicle_ID\"] not in data_dict:\n",
        "            data_dict[data_i[\"Vehicle_ID\"]] = {}\n",
        "        data_dict[data_i.Vehicle_ID][data_i.Global_Time] = (data_i.Local_X,\n",
        "                                                            data_i.Local_Y,\n",
        "                                                            data_i.v_length,\n",
        "                                                            data_i.v_Vel, \n",
        "                                                            data_i.v_Acc,\n",
        "                                                            data_i.Lane_ID,\n",
        "                                                            data_i.Space_Headway\n",
        "                                                            )\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "def load_data_dict(data=None, convert=False, path_to_file=None):\n",
        "    \"\"\"Returns data dictionary.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    convert : boolean\n",
        "        - True if data should be converted to dictionary and saved as json.\n",
        "        - False if dict data should be loaded from a json file.\n",
        "    data : pandas.Dataframe\n",
        "        pandas dataframe containing NGSIM dataset\n",
        "    path_to_file : str\n",
        "        path to the json file\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    data_dict : dict\n",
        "        a dictinoary containing NGSIM data\n",
        "    \"\"\"\n",
        "    if convert:\n",
        "        data_dict = convert_to_dict(data)\n",
        "        json_file = json.dumps(data_dict)\n",
        "        with open(path_to_file, 'w') as f:       \n",
        "            f.write(json_file)\n",
        "    else:\n",
        "        with open(path_to_file, 'r') as f:\n",
        "            data_dict = json.load(f)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "\n",
        "def collect_lead_lag_info(data, data_dict_lead_info):\n",
        "    \"\"\"Returns data dictionary with additional columns for lead vehicle info.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pandas.Dataframe\n",
        "        pandas dataframe containing NGSIM dataset\n",
        "    path_to_file : str\n",
        "        path to json file\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data_dict : dict\n",
        "        a dictinoary containing NGSIM data with additional columns:\n",
        "          - Local_X_leader : Local_X of the leading vehicle\n",
        "          - Local_Y_leader : Local_Y of the leading vehicle\n",
        "          - v_length_leader : v_length (length) of the leading vehicle\n",
        "          - v_Vel_leader : v_Vel (speed) of the leading vehicle\n",
        "          - v_Acc_leader : v_Acc (acceleration) of the leading vehicle\n",
        "          - Gap_leader : bumper to bumper gap to the leading vehicle\n",
        "\n",
        "    \"\"\"\n",
        "    initial_time = time.time()\n",
        "    # data['Gap'] = 300.  # if no lead vehicle, set to 300 ft\n",
        "    # data['Lead_Speed'] = 130.  # if no lead vehicle, set to 130 ft/s\n",
        "    # data['Lead_Length'] = 130.  # if no lead vehicle, set to 130 ft/s\n",
        "\n",
        "    print(\"=== Collecting lead and lag info ===\")\n",
        "\n",
        "    total_length = data.shape[0]\n",
        "    count = 0\n",
        "    for index, data_i in data.iterrows():\n",
        "        if count % 50000 == 0:\n",
        "            print('index: {}, count: {}, of: {}, time_passed: {}'.format(\n",
        "                    index, count, total_length, time.time()-initial_time))\n",
        "            \n",
        "        # Lead vehicle info\n",
        "        if data_i.Preceding != 0:\n",
        "            try:\n",
        "                x_l, y_l, l_l, v_l, a_l, lane_l, sh_l = data_dict_lead_info['{}'.format(data_i.Preceding)]['{}'.format(data_i.Global_Time)]\n",
        "                data.at[index, 'Local_X_leader'] = x_l\n",
        "                data.at[index, 'Local_Y_leader'] = y_l\n",
        "                data.at[index, 'v_length_leader'] = l_l\n",
        "                data.at[index, 'v_Vel_leader'] = v_l\n",
        "                data.at[index, 'v_Acc_leader'] = a_l\n",
        "                data.at[index, 'Lane_ID_leader'] = lane_l\n",
        "\n",
        "                # Compute bumper to bumper gap (space headway minus length)\n",
        "                if data_i[\"Space_Headway\"] > 0:\n",
        "                    data.at[index, 'Gap_leader'] = data_i['Space_Headway'] - l_l\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Following vehicle info\n",
        "        if data_i.Following != 0:\n",
        "            try:\n",
        "                x_f, y_f, l_f, v_f, a_f, lane_f, sh_f = data_dict_lead_info['{}'.format(data_i.Following)]['{}'.format(data_i.Global_Time)]\n",
        "                data.at[index, 'Local_X_follower'] = x_f\n",
        "                data.at[index, 'Local_Y_follower'] = y_f\n",
        "                data.at[index, 'v_length_follower'] = l_f\n",
        "                data.at[index, 'v_Vel_follower'] = v_f\n",
        "                data.at[index, 'v_Acc_follower'] = a_f\n",
        "                data.at[index, 'Lane_ID_follower'] = lane_f\n",
        "                data.at[index, 'Space_Headway_follower'] = sh_f\n",
        "\n",
        "                # Compute bumper to bumper gap (space headway minus length)\n",
        "                if sh_f > 0:\n",
        "                    data.at[index, 'Gap_follower'] = sh_f - data_i['v_length']\n",
        "            except:\n",
        "                pass\n",
        "        count += 1\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnbiI9-UCK66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_prepare():\n",
        "    \"\"\"Import and break apart NGSIM data.\"\"\"\n",
        "    print(\"=== Importing NGSIM data ===\")\n",
        "    data = load_data(download=True)\n",
        "\n",
        "    # filter by location for memory purposes \n",
        "    locations = ['us-101', 'i-80']  # ['peachtree', 'lankershim', 'us-101', 'i-80'] \n",
        "\n",
        "    for location in locations:\n",
        "        # save to csv\n",
        "        print(\"++++++++++++ Location: {} +++++++++++++++ \".format(location))\n",
        "        data[data.Location==location].to_csv('{}.csv'.format(location), index=False)\n",
        "\n",
        "def main_to_dict_lead():\n",
        "    locations = ['us-101', 'i-80']   \n",
        "    for location in locations:\n",
        "        print(\"++++++++++++ Location: {} +++++++++++++++ \".format(location))\n",
        "\n",
        "        # Load the dataset of the specific location.\n",
        "        data = pd.read_csv(\"{}.csv\".format(location))\n",
        "\n",
        "        # Convert to and save dict version.\n",
        "        path_to_file = '{}-lead.json'.format(location)\n",
        "        data_dict_lead = load_data_dict(data, convert=True, path_to_file=path_to_file)\n",
        "\n",
        "\n",
        "def main_lead_lag_data():\n",
        "    locations = ['us-101', 'i-80']  \n",
        "    for location in locations:\n",
        "        print(\"++++++++++++ Location: {} +++++++++++++++ \".format(location))\n",
        "\n",
        "        # Load the dataset of the specific location.\n",
        "        print(\"=== Loading saved information. ===\")\n",
        "        data = pd.read_csv(\"{}.csv\".format(location))\n",
        "        path_to_file = '{}-lead.json'.format(location)\n",
        "        data_dict_lead = load_data_dict(None, convert=False, path_to_file=path_to_file)\n",
        "\n",
        "        # lead and following vehicle info\n",
        "        print(\"=== Collecting leading and following vehicle info ===\")\n",
        "        data = collect_lead_lag_info(data, data_dict_lead)\n",
        "\n",
        "        print(\"=== Exporting processed data ===\")\n",
        "        data.to_csv('processed_{}.csv'.format(location), index=False)  # FIXME\n",
        "\n",
        "def main_split_data():\n",
        "    \"\"\"Split the date set into free flow and car-following regimes.\n",
        "    Split is based on the space headway. If space headway is larger than 200 m,\n",
        "    free flow regime is assigned.\n",
        "    \"\"\"\n",
        "    locations = ['us-101', 'i-80']  \n",
        "    for location in locations:\n",
        "        print(\"++++++++++++ Location: {} +++++++++++++++ \".format(location))\n",
        "\n",
        "        # Load the dataset of the specific location.\n",
        "        print(\"=== Loading saved information. ===\")\n",
        "        data = pd.read_csv('processed_{}.csv'.format(location))\n",
        "\n",
        "        # Split: filter free flow if the headway is larger than 200 m (656 ft)\n",
        "        # and if therer is no leading vehicle\n",
        "\n",
        "        data_cf = data[(data[\"Space_Headway\"] < 656) & (data[\"Preceding\"] != 0)]\n",
        "        data_ff = data[(data[\"Space_Headway\"] >= 656) | (data[\"Preceding\"] == 0)]\n",
        "\n",
        "        # sort data by vehicle id and time\n",
        "        data_cf = data_cf.sort_values(by=['Vehicle_ID','Global_Time'])\n",
        "        data_ff = data_ff.sort_values(by=['Vehicle_ID','Global_Time'])\n",
        "\n",
        "        print(\"=== Exporting processed data ===\")\n",
        "        data_ff.to_csv('/content/drive/My Drive/NGSIM_Aimsun/processed_free_flow_{}.csv'.format(location), index=False)\n",
        "        data_cf.to_csv('/content/drive/My Drive/NGSIM_Aimsun/processed_car_following_{}.csv'.format(location), index=False)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYv-8hMCCPpB",
        "colab_type": "code",
        "outputId": "63ff88e8-4042-4130-b859-68ffb4109b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "t0 = time.time()\n",
        "main_prepare()\n",
        "print(\"Done! Took %.3f seconds\" %(time.time() - t0))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Importing NGSIM data ===\n",
            "#### Mounting google colab ###########\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "++++++++++++ Location: us-101 +++++++++++++++ \n",
            "++++++++++++ Location: i-80 +++++++++++++++ \n",
            "Done! Took 235.506 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhDPnIARCT7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c79d74d-24e4-4fb3-80bd-c17d8980fded"
      },
      "source": [
        "t1 = time.time()\n",
        "data = main_to_dict_lead()\n",
        "print(\"Done! Took %.3f seconds\" %(time.time() - t1))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++ Location: us-101 +++++++++++++++ \n",
            "=== Coverting relavent data to dictionary ===\n",
            "index:  0\n",
            "index:  50000\n",
            "index:  100000\n",
            "index:  150000\n",
            "index:  200000\n",
            "index:  250000\n",
            "index:  300000\n",
            "index:  350000\n",
            "index:  400000\n",
            "index:  450000\n",
            "index:  500000\n",
            "index:  550000\n",
            "index:  600000\n",
            "index:  650000\n",
            "index:  700000\n",
            "index:  750000\n",
            "index:  800000\n",
            "index:  850000\n",
            "index:  900000\n",
            "index:  950000\n",
            "index:  1000000\n",
            "index:  1050000\n",
            "index:  1100000\n",
            "index:  1150000\n",
            "index:  1200000\n",
            "index:  1250000\n",
            "index:  1300000\n",
            "index:  1350000\n",
            "index:  1400000\n",
            "index:  1450000\n",
            "index:  1500000\n",
            "index:  1550000\n",
            "index:  1600000\n",
            "index:  1650000\n",
            "index:  1700000\n",
            "index:  1750000\n",
            "index:  1800000\n",
            "index:  1850000\n",
            "index:  1900000\n",
            "index:  1950000\n",
            "index:  2000000\n",
            "index:  2050000\n",
            "index:  2100000\n",
            "index:  2150000\n",
            "index:  2200000\n",
            "index:  2250000\n",
            "index:  2300000\n",
            "index:  2350000\n",
            "index:  2400000\n",
            "index:  2450000\n",
            "index:  2500000\n",
            "index:  2550000\n",
            "index:  2600000\n",
            "index:  2650000\n",
            "index:  2700000\n",
            "index:  2750000\n",
            "index:  2800000\n",
            "index:  2850000\n",
            "index:  2900000\n",
            "index:  2950000\n",
            "index:  3000000\n",
            "index:  3050000\n",
            "index:  3100000\n",
            "index:  3150000\n",
            "index:  3200000\n",
            "index:  3250000\n",
            "index:  3300000\n",
            "index:  3350000\n",
            "index:  3400000\n",
            "index:  3450000\n",
            "index:  3500000\n",
            "index:  3550000\n",
            "index:  3600000\n",
            "index:  3650000\n",
            "index:  3700000\n",
            "index:  3750000\n",
            "index:  3800000\n",
            "index:  3850000\n",
            "index:  3900000\n",
            "index:  3950000\n",
            "index:  4000000\n",
            "index:  4050000\n",
            "++++++++++++ Location: i-80 +++++++++++++++ \n",
            "=== Coverting relavent data to dictionary ===\n",
            "index:  0\n",
            "index:  50000\n",
            "index:  100000\n",
            "index:  150000\n",
            "index:  200000\n",
            "index:  250000\n",
            "index:  300000\n",
            "index:  350000\n",
            "index:  400000\n",
            "index:  450000\n",
            "index:  500000\n",
            "index:  550000\n",
            "index:  600000\n",
            "index:  650000\n",
            "index:  700000\n",
            "index:  750000\n",
            "index:  800000\n",
            "index:  850000\n",
            "index:  900000\n",
            "index:  950000\n",
            "index:  1000000\n",
            "index:  1050000\n",
            "index:  1100000\n",
            "index:  1150000\n",
            "index:  1200000\n",
            "index:  1250000\n",
            "index:  1300000\n",
            "index:  1350000\n",
            "index:  1400000\n",
            "index:  1450000\n",
            "index:  1500000\n",
            "index:  1550000\n",
            "index:  1600000\n",
            "index:  1650000\n",
            "index:  1700000\n",
            "index:  1750000\n",
            "index:  1800000\n",
            "index:  1850000\n",
            "index:  1900000\n",
            "index:  1950000\n",
            "index:  2000000\n",
            "index:  2050000\n",
            "index:  2100000\n",
            "index:  2150000\n",
            "index:  2200000\n",
            "index:  2250000\n",
            "index:  2300000\n",
            "index:  2350000\n",
            "index:  2400000\n",
            "index:  2450000\n",
            "index:  2500000\n",
            "index:  2550000\n",
            "index:  2600000\n",
            "index:  2650000\n",
            "index:  2700000\n",
            "index:  2750000\n",
            "index:  2800000\n",
            "index:  2850000\n",
            "index:  2900000\n",
            "index:  2950000\n",
            "index:  3000000\n",
            "index:  3050000\n",
            "index:  3100000\n",
            "index:  3150000\n",
            "index:  3200000\n",
            "index:  3250000\n",
            "index:  3300000\n",
            "index:  3350000\n",
            "index:  3400000\n",
            "index:  3450000\n",
            "index:  3500000\n",
            "index:  3550000\n",
            "index:  3600000\n",
            "index:  3650000\n",
            "index:  3700000\n",
            "index:  3750000\n",
            "index:  3800000\n",
            "index:  3850000\n",
            "index:  3900000\n",
            "index:  3950000\n",
            "index:  4000000\n",
            "index:  4050000\n",
            "index:  4100000\n",
            "index:  4150000\n",
            "index:  4200000\n",
            "index:  4250000\n",
            "index:  4300000\n",
            "index:  4350000\n",
            "index:  4400000\n",
            "index:  4450000\n",
            "index:  4500000\n",
            "index:  4550000\n",
            "Done! Took 1799.120 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HczlgUYuCXM7",
        "colab_type": "code",
        "outputId": "787d669b-8a81-4a3e-d919-146e947d707c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "t2 = time.time()\n",
        "data = main_lead_lag_data()\n",
        "print(\"Done! Took %.3f seconds\" %(time.time() - t2))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++ Location: us-101 +++++++++++++++ \n",
            "=== Loading saved information. ===\n",
            "=== Collecting leading and following vehicle info ===\n",
            "=== Collecting lead and lag info ===\n",
            "index: 0, count: 0, of: 4098933, time_passed: 3.9248156547546387\n",
            "index: 50000, count: 50000, of: 4098933, time_passed: 26.326353311538696\n",
            "index: 100000, count: 100000, of: 4098933, time_passed: 43.08476734161377\n",
            "index: 150000, count: 150000, of: 4098933, time_passed: 62.79113149642944\n",
            "index: 200000, count: 200000, of: 4098933, time_passed: 84.45963978767395\n",
            "index: 250000, count: 250000, of: 4098933, time_passed: 107.28442859649658\n",
            "index: 300000, count: 300000, of: 4098933, time_passed: 130.3248643875122\n",
            "index: 350000, count: 350000, of: 4098933, time_passed: 153.52926921844482\n",
            "index: 400000, count: 400000, of: 4098933, time_passed: 177.49186897277832\n",
            "index: 450000, count: 450000, of: 4098933, time_passed: 200.78222489356995\n",
            "index: 500000, count: 500000, of: 4098933, time_passed: 226.34674906730652\n",
            "index: 550000, count: 550000, of: 4098933, time_passed: 249.8932135105133\n",
            "index: 600000, count: 600000, of: 4098933, time_passed: 273.6672782897949\n",
            "index: 650000, count: 650000, of: 4098933, time_passed: 297.9007043838501\n",
            "index: 700000, count: 700000, of: 4098933, time_passed: 321.49062061309814\n",
            "index: 750000, count: 750000, of: 4098933, time_passed: 345.76686000823975\n",
            "index: 800000, count: 800000, of: 4098933, time_passed: 369.53262591362\n",
            "index: 850000, count: 850000, of: 4098933, time_passed: 393.4897549152374\n",
            "index: 900000, count: 900000, of: 4098933, time_passed: 417.79305768013\n",
            "index: 950000, count: 950000, of: 4098933, time_passed: 441.20907640457153\n",
            "index: 1000000, count: 1000000, of: 4098933, time_passed: 465.54804611206055\n",
            "index: 1050000, count: 1050000, of: 4098933, time_passed: 489.3575623035431\n",
            "index: 1100000, count: 1100000, of: 4098933, time_passed: 513.2463347911835\n",
            "index: 1150000, count: 1150000, of: 4098933, time_passed: 537.2190115451813\n",
            "index: 1200000, count: 1200000, of: 4098933, time_passed: 560.6343412399292\n",
            "index: 1250000, count: 1250000, of: 4098933, time_passed: 584.2442998886108\n",
            "index: 1300000, count: 1300000, of: 4098933, time_passed: 607.9997622966766\n",
            "index: 1350000, count: 1350000, of: 4098933, time_passed: 631.7712302207947\n",
            "index: 1400000, count: 1400000, of: 4098933, time_passed: 655.6479132175446\n",
            "index: 1450000, count: 1450000, of: 4098933, time_passed: 678.8766984939575\n",
            "index: 1500000, count: 1500000, of: 4098933, time_passed: 702.8725502490997\n",
            "index: 1550000, count: 1550000, of: 4098933, time_passed: 726.4190514087677\n",
            "index: 1600000, count: 1600000, of: 4098933, time_passed: 750.2969589233398\n",
            "index: 1650000, count: 1650000, of: 4098933, time_passed: 774.1436295509338\n",
            "index: 1700000, count: 1700000, of: 4098933, time_passed: 797.7227594852448\n",
            "index: 1750000, count: 1750000, of: 4098933, time_passed: 821.9498233795166\n",
            "index: 1800000, count: 1800000, of: 4098933, time_passed: 845.8771343231201\n",
            "index: 1850000, count: 1850000, of: 4098933, time_passed: 869.9424827098846\n",
            "index: 1900000, count: 1900000, of: 4098933, time_passed: 894.0936846733093\n",
            "index: 1950000, count: 1950000, of: 4098933, time_passed: 917.8225746154785\n",
            "index: 2000000, count: 2000000, of: 4098933, time_passed: 942.4779586791992\n",
            "index: 2050000, count: 2050000, of: 4098933, time_passed: 966.6997146606445\n",
            "index: 2100000, count: 2100000, of: 4098933, time_passed: 990.2984254360199\n",
            "index: 2150000, count: 2150000, of: 4098933, time_passed: 1014.6279220581055\n",
            "index: 2200000, count: 2200000, of: 4098933, time_passed: 1039.1557657718658\n",
            "index: 2250000, count: 2250000, of: 4098933, time_passed: 1063.52303647995\n",
            "index: 2300000, count: 2300000, of: 4098933, time_passed: 1087.7442755699158\n",
            "index: 2350000, count: 2350000, of: 4098933, time_passed: 1111.422026872635\n",
            "index: 2400000, count: 2400000, of: 4098933, time_passed: 1134.9793045520782\n",
            "index: 2450000, count: 2450000, of: 4098933, time_passed: 1158.8232626914978\n",
            "index: 2500000, count: 2500000, of: 4098933, time_passed: 1182.8474299907684\n",
            "index: 2550000, count: 2550000, of: 4098933, time_passed: 1207.1401607990265\n",
            "index: 2600000, count: 2600000, of: 4098933, time_passed: 1231.5876586437225\n",
            "index: 2650000, count: 2650000, of: 4098933, time_passed: 1256.1139764785767\n",
            "index: 2700000, count: 2700000, of: 4098933, time_passed: 1279.6131422519684\n",
            "index: 2750000, count: 2750000, of: 4098933, time_passed: 1303.2092080116272\n",
            "index: 2800000, count: 2800000, of: 4098933, time_passed: 1326.657015800476\n",
            "index: 2850000, count: 2850000, of: 4098933, time_passed: 1350.3185284137726\n",
            "index: 2900000, count: 2900000, of: 4098933, time_passed: 1373.8083131313324\n",
            "index: 2950000, count: 2950000, of: 4098933, time_passed: 1397.1148419380188\n",
            "index: 3000000, count: 3000000, of: 4098933, time_passed: 1421.7111330032349\n",
            "index: 3050000, count: 3050000, of: 4098933, time_passed: 1445.7635006904602\n",
            "index: 3100000, count: 3100000, of: 4098933, time_passed: 1470.2413594722748\n",
            "index: 3150000, count: 3150000, of: 4098933, time_passed: 1494.32173538208\n",
            "index: 3200000, count: 3200000, of: 4098933, time_passed: 1518.4271092414856\n",
            "index: 3250000, count: 3250000, of: 4098933, time_passed: 1542.6110708713531\n",
            "index: 3300000, count: 3300000, of: 4098933, time_passed: 1566.802288532257\n",
            "index: 3350000, count: 3350000, of: 4098933, time_passed: 1591.6826102733612\n",
            "index: 3400000, count: 3400000, of: 4098933, time_passed: 1615.7265722751617\n",
            "index: 3450000, count: 3450000, of: 4098933, time_passed: 1640.275018453598\n",
            "index: 3500000, count: 3500000, of: 4098933, time_passed: 1664.524887084961\n",
            "index: 3550000, count: 3550000, of: 4098933, time_passed: 1688.6678702831268\n",
            "index: 3600000, count: 3600000, of: 4098933, time_passed: 1713.4318356513977\n",
            "index: 3650000, count: 3650000, of: 4098933, time_passed: 1737.8693578243256\n",
            "index: 3700000, count: 3700000, of: 4098933, time_passed: 1761.8143780231476\n",
            "index: 3750000, count: 3750000, of: 4098933, time_passed: 1785.971580505371\n",
            "index: 3800000, count: 3800000, of: 4098933, time_passed: 1810.0806376934052\n",
            "index: 3850000, count: 3850000, of: 4098933, time_passed: 1834.0554184913635\n",
            "index: 3900000, count: 3900000, of: 4098933, time_passed: 1857.4652664661407\n",
            "index: 3950000, count: 3950000, of: 4098933, time_passed: 1879.2657451629639\n",
            "index: 4000000, count: 4000000, of: 4098933, time_passed: 1899.309484243393\n",
            "index: 4050000, count: 4050000, of: 4098933, time_passed: 1916.9583990573883\n",
            "=== Exporting processed data ===\n",
            "++++++++++++ Location: i-80 +++++++++++++++ \n",
            "=== Loading saved information. ===\n",
            "=== Collecting leading and following vehicle info ===\n",
            "=== Collecting lead and lag info ===\n",
            "index: 0, count: 0, of: 4566387, time_passed: 5.308543682098389\n",
            "index: 50000, count: 50000, of: 4566387, time_passed: 30.85358452796936\n",
            "index: 100000, count: 100000, of: 4566387, time_passed: 47.07423520088196\n",
            "index: 150000, count: 150000, of: 4566387, time_passed: 65.84710955619812\n",
            "index: 200000, count: 200000, of: 4566387, time_passed: 87.07831001281738\n",
            "index: 250000, count: 250000, of: 4566387, time_passed: 109.29012060165405\n",
            "index: 300000, count: 300000, of: 4566387, time_passed: 132.2788393497467\n",
            "index: 350000, count: 350000, of: 4566387, time_passed: 155.71364879608154\n",
            "index: 400000, count: 400000, of: 4566387, time_passed: 178.56220817565918\n",
            "index: 450000, count: 450000, of: 4566387, time_passed: 202.2835762500763\n",
            "index: 500000, count: 500000, of: 4566387, time_passed: 229.36744022369385\n",
            "index: 550000, count: 550000, of: 4566387, time_passed: 253.9425630569458\n",
            "index: 600000, count: 600000, of: 4566387, time_passed: 277.53405261039734\n",
            "index: 650000, count: 650000, of: 4566387, time_passed: 300.77547097206116\n",
            "index: 700000, count: 700000, of: 4566387, time_passed: 324.62981629371643\n",
            "index: 750000, count: 750000, of: 4566387, time_passed: 347.5500237941742\n",
            "index: 800000, count: 800000, of: 4566387, time_passed: 370.2865357398987\n",
            "index: 850000, count: 850000, of: 4566387, time_passed: 393.8014633655548\n",
            "index: 900000, count: 900000, of: 4566387, time_passed: 416.89080023765564\n",
            "index: 950000, count: 950000, of: 4566387, time_passed: 440.8152678012848\n",
            "index: 1000000, count: 1000000, of: 4566387, time_passed: 464.4346852302551\n",
            "index: 1050000, count: 1050000, of: 4566387, time_passed: 487.7093324661255\n",
            "index: 1100000, count: 1100000, of: 4566387, time_passed: 511.7676010131836\n",
            "index: 1150000, count: 1150000, of: 4566387, time_passed: 535.7098441123962\n",
            "index: 1200000, count: 1200000, of: 4566387, time_passed: 559.375604391098\n",
            "index: 1250000, count: 1250000, of: 4566387, time_passed: 582.9731514453888\n",
            "index: 1300000, count: 1300000, of: 4566387, time_passed: 606.5098996162415\n",
            "index: 1350000, count: 1350000, of: 4566387, time_passed: 630.6224789619446\n",
            "index: 1400000, count: 1400000, of: 4566387, time_passed: 654.1440484523773\n",
            "index: 1450000, count: 1450000, of: 4566387, time_passed: 679.0312156677246\n",
            "index: 1500000, count: 1500000, of: 4566387, time_passed: 702.6050372123718\n",
            "index: 1550000, count: 1550000, of: 4566387, time_passed: 725.8082070350647\n",
            "index: 1600000, count: 1600000, of: 4566387, time_passed: 749.7170550823212\n",
            "index: 1650000, count: 1650000, of: 4566387, time_passed: 773.0959668159485\n",
            "index: 1700000, count: 1700000, of: 4566387, time_passed: 796.6989736557007\n",
            "index: 1750000, count: 1750000, of: 4566387, time_passed: 819.7256467342377\n",
            "index: 1800000, count: 1800000, of: 4566387, time_passed: 842.5734252929688\n",
            "index: 1850000, count: 1850000, of: 4566387, time_passed: 866.3632442951202\n",
            "index: 1900000, count: 1900000, of: 4566387, time_passed: 890.2796635627747\n",
            "index: 1950000, count: 1950000, of: 4566387, time_passed: 914.5901529788971\n",
            "index: 2000000, count: 2000000, of: 4566387, time_passed: 939.1640367507935\n",
            "index: 2050000, count: 2050000, of: 4566387, time_passed: 963.3673257827759\n",
            "index: 2100000, count: 2100000, of: 4566387, time_passed: 987.976114988327\n",
            "index: 2150000, count: 2150000, of: 4566387, time_passed: 1012.4943277835846\n",
            "index: 2200000, count: 2200000, of: 4566387, time_passed: 1036.7219021320343\n",
            "index: 2250000, count: 2250000, of: 4566387, time_passed: 1060.9443185329437\n",
            "index: 2300000, count: 2300000, of: 4566387, time_passed: 1085.2444665431976\n",
            "index: 2350000, count: 2350000, of: 4566387, time_passed: 1109.9261229038239\n",
            "index: 2400000, count: 2400000, of: 4566387, time_passed: 1133.7442469596863\n",
            "index: 2450000, count: 2450000, of: 4566387, time_passed: 1157.6831681728363\n",
            "index: 2500000, count: 2500000, of: 4566387, time_passed: 1181.6211318969727\n",
            "index: 2550000, count: 2550000, of: 4566387, time_passed: 1205.9820864200592\n",
            "index: 2600000, count: 2600000, of: 4566387, time_passed: 1230.5847470760345\n",
            "index: 2650000, count: 2650000, of: 4566387, time_passed: 1254.759377002716\n",
            "index: 2700000, count: 2700000, of: 4566387, time_passed: 1279.3104708194733\n",
            "index: 2750000, count: 2750000, of: 4566387, time_passed: 1303.2199020385742\n",
            "index: 2800000, count: 2800000, of: 4566387, time_passed: 1327.4983048439026\n",
            "index: 2850000, count: 2850000, of: 4566387, time_passed: 1351.9453995227814\n",
            "index: 2900000, count: 2900000, of: 4566387, time_passed: 1375.6455886363983\n",
            "index: 2950000, count: 2950000, of: 4566387, time_passed: 1399.8090035915375\n",
            "index: 3000000, count: 3000000, of: 4566387, time_passed: 1423.788197517395\n",
            "index: 3050000, count: 3050000, of: 4566387, time_passed: 1448.2970881462097\n",
            "index: 3100000, count: 3100000, of: 4566387, time_passed: 1472.8506302833557\n",
            "index: 3150000, count: 3150000, of: 4566387, time_passed: 1496.943280696869\n",
            "index: 3200000, count: 3200000, of: 4566387, time_passed: 1522.1270551681519\n",
            "index: 3250000, count: 3250000, of: 4566387, time_passed: 1546.3124265670776\n",
            "index: 3300000, count: 3300000, of: 4566387, time_passed: 1570.6452686786652\n",
            "index: 3350000, count: 3350000, of: 4566387, time_passed: 1595.0073199272156\n",
            "index: 3400000, count: 3400000, of: 4566387, time_passed: 1619.8223836421967\n",
            "index: 3450000, count: 3450000, of: 4566387, time_passed: 1644.237329006195\n",
            "index: 3500000, count: 3500000, of: 4566387, time_passed: 1668.019314289093\n",
            "index: 3550000, count: 3550000, of: 4566387, time_passed: 1691.8141503334045\n",
            "index: 3600000, count: 3600000, of: 4566387, time_passed: 1715.9453101158142\n",
            "index: 3650000, count: 3650000, of: 4566387, time_passed: 1739.760731458664\n",
            "index: 3700000, count: 3700000, of: 4566387, time_passed: 1764.289885044098\n",
            "index: 3750000, count: 3750000, of: 4566387, time_passed: 1788.174191236496\n",
            "index: 3800000, count: 3800000, of: 4566387, time_passed: 1812.4295325279236\n",
            "index: 3850000, count: 3850000, of: 4566387, time_passed: 1836.8100740909576\n",
            "index: 3900000, count: 3900000, of: 4566387, time_passed: 1860.7349717617035\n",
            "index: 3950000, count: 3950000, of: 4566387, time_passed: 1885.1822612285614\n",
            "index: 4000000, count: 4000000, of: 4566387, time_passed: 1909.5523586273193\n",
            "index: 4050000, count: 4050000, of: 4566387, time_passed: 1933.3824424743652\n",
            "index: 4100000, count: 4100000, of: 4566387, time_passed: 1957.4544558525085\n",
            "index: 4150000, count: 4150000, of: 4566387, time_passed: 1981.0152623653412\n",
            "index: 4200000, count: 4200000, of: 4566387, time_passed: 2005.182326078415\n",
            "index: 4250000, count: 4250000, of: 4566387, time_passed: 2028.9968638420105\n",
            "index: 4300000, count: 4300000, of: 4566387, time_passed: 2052.7821266651154\n",
            "index: 4350000, count: 4350000, of: 4566387, time_passed: 2075.658976316452\n",
            "index: 4400000, count: 4400000, of: 4566387, time_passed: 2097.2251303195953\n",
            "index: 4450000, count: 4450000, of: 4566387, time_passed: 2117.3333837985992\n",
            "index: 4500000, count: 4500000, of: 4566387, time_passed: 2134.718738794327\n",
            "index: 4550000, count: 4550000, of: 4566387, time_passed: 2150.632145881653\n",
            "=== Exporting processed data ===\n",
            "Done! Took 4464.211 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJrt0s_qFk4s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ea108588-d755-425e-f3b1-687fc241336f"
      },
      "source": [
        "t3 = time.time()\n",
        "data = main_split_data()\n",
        "print(\"Done! Took %.3f seconds\" %(time.time() - t3))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "++++++++++++ Location: us-101 +++++++++++++++ \n",
            "=== Loading saved information. ===\n",
            "=== Exporting processed data ===\n",
            "++++++++++++ Location: i-80 +++++++++++++++ \n",
            "=== Loading saved information. ===\n",
            "=== Exporting processed data ===\n",
            "Done! Took 408.911 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7mGS12JGxg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}